# IntÃ©gration Airflow - LogiData AI

## ğŸ¯ Vue d'ensemble

Cette documentation dÃ©crit l'intÃ©gration complÃ¨te d'Apache Airflow dans LogiData AI pour l'orchestration et l'exÃ©cution des pipelines ETL/ELT.

## ğŸ“ Fichiers crÃ©Ã©s

### 1. Client Airflow API
**Fichier:** `backend/app/integrations/airflow_client.py`

Wrapper pour l'API REST d'Airflow qui permet :
- âœ… DÃ©clencher des DAGs
- âœ… RÃ©cupÃ©rer le statut d'exÃ©cution
- âœ… Annuler des exÃ©cutions
- âœ… GÃ©rer les DAGs (pause/unpause)
- âœ… RÃ©cupÃ©rer les logs (Ã  implÃ©menter complÃ¨tement)

**Classe principale:** `AirflowClient`

**Singleton:** `get_airflow_client()` pour une instance globale

### 2. GÃ©nÃ©rateur de DAGs dynamiques
**Fichier:** `backend/app/airflow/dag_generator.py`

GÃ©nÃ¨re automatiquement des DAGs Airflow Ã  partir des configurations de pipelines :
- âœ… Convertit la config JSON (nodes + edges) en code Python Airflow
- âœ… CrÃ©e les fichiers DAG dans `/backend/dags/`
- âœ… GÃ¨re les dÃ©pendances entre tÃ¢ches
- âœ… Supporte les schedules (cron expressions)
- âœ… Met Ã  jour les DAGs existants

**Classe principale:** `DAGGenerator`

### 3. OpÃ©rateur Airflow personnalisÃ©
**Fichier:** `backend/dags/operators/etl_operator.py`

OpÃ©rateur Airflow custom pour exÃ©cuter les modules ETL :
- âœ… Charge dynamiquement les classes de modules (extractors, transformers, loaders)
- âœ… GÃ¨re le passage de donnÃ©es via XCom entre tÃ¢ches
- âœ… Convertit DataFrames en dict pour la sÃ©rialisation
- âœ… GÃ¨re les erreurs et logs
- âœ… Couleurs UI diffÃ©rentes par type de module

**Classe principale:** `ETLOperator`

### 4. TÃ¢ches Celery mises Ã  jour
**Fichier:** `backend/app/workers/tasks/pipeline.py`

TÃ¢ches asynchrones pour l'orchestration :

#### `execute_pipeline(pipeline_id, params, trigger_type, user_id)`
- Charge la config du pipeline depuis la DB
- CrÃ©e un enregistrement d'exÃ©cution
- GÃ©nÃ¨re/met Ã  jour le DAG Airflow
- DÃ©clenche l'exÃ©cution via l'API Airflow
- Met Ã  jour le statut en base

#### `monitor_execution(execution_id)`
- Interroge l'API Airflow pour obtenir le statut
- Met Ã  jour l'enregistrement d'exÃ©cution
- Calcule la durÃ©e d'exÃ©cution
- Map les Ã©tats Airflow vers nos Ã©tats

#### `cancel_pipeline(pipeline_id, execution_id)`
- Annule l'exÃ©cution Airflow via API
- Met Ã  jour le statut Ã  "cancelled"

### 5. Endpoints API mis Ã  jour
**Fichier:** `backend/app/api/v1/pipelines.py`

#### `POST /api/v1/pipelines/{pipeline_id}/execute`
- DÃ©clenche l'exÃ©cution via Celery
- Retourne le task_id Celery

**Fichier:** `backend/app/api/v1/executions.py`

#### `GET /api/v1/executions`
- Liste les exÃ©cutions avec pagination et filtres

#### `GET /api/v1/executions/{execution_id}`
- RÃ©cupÃ¨re les dÃ©tails d'une exÃ©cution

#### `POST /api/v1/executions/{execution_id}/monitor`
- DÃ©clenche une mise Ã  jour du statut depuis Airflow

#### `POST /api/v1/executions/{execution_id}/cancel`
- Annule une exÃ©cution en cours

#### `GET /api/v1/executions/{execution_id}/logs`
- RÃ©cupÃ¨re les logs d'exÃ©cution

## ğŸ”„ Workflow d'exÃ©cution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. User clicks "Execute" in UI                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. POST /api/v1/pipelines/{id}/execute                       â”‚
â”‚    - FastAPI endpoint receives request                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Celery task: execute_pipeline.delay()                     â”‚
â”‚    - Task submitted to Redis queue                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Celery Worker picks up task                               â”‚
â”‚    - Load pipeline config from PostgreSQL                    â”‚
â”‚    - Create PipelineExecution record (status: pending)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. DAGGenerator.update_dag()                                  â”‚
â”‚    - Generate Python DAG file from config                    â”‚
â”‚    - Write to /backend/dags/pipeline_{uuid}.py               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Airflow Scheduler detects new DAG                         â”‚
â”‚    - Parse DAG file                                           â”‚
â”‚    - Register tasks and dependencies                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. AirflowClient.trigger_dag()                                â”‚
â”‚    - POST to Airflow API /api/v1/dags/{dag_id}/dagRuns      â”‚
â”‚    - Pass pipeline config and params                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. Airflow executes DAG                                       â”‚
â”‚    - Airflow Worker runs ETLOperator tasks                   â”‚
â”‚    - Each task:                                               â”‚
â”‚      * Loads module class dynamically                        â”‚
â”‚      * Executes module.execute()                             â”‚
â”‚      * Passes data via XCom to next task                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. Monitoring (optional periodic polling)                    â”‚
â”‚    - POST /api/v1/executions/{id}/monitor                    â”‚
â”‚    - Celery task: monitor_execution.delay()                  â”‚
â”‚    - AirflowClient.get_dag_run_status()                       â”‚
â”‚    - Update PipelineExecution status in DB                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 10. Completion                                                â”‚
â”‚     - Airflow marks DAG as success/failed                    â”‚
â”‚     - Monitor task updates final status                      â”‚
â”‚     - User sees result in UI                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Configuration requise

### Variables d'environnement (backend/.env)

```env
# Airflow Configuration
AIRFLOW_API_URL=http://airflow-webserver:8080/api/v1
AIRFLOW_USERNAME=admin
AIRFLOW_PASSWORD=admin
AIRFLOW_HOME=/opt/airflow
AIRFLOW_WEBSERVER_PORT=8080
```

### Services Docker Compose

Les services suivants doivent Ãªtre actifs :
- `airflow-postgres` : Base de donnÃ©es mÃ©tadonnÃ©es Airflow
- `airflow-webserver` : Interface web Airflow (port 8080)
- `airflow-scheduler` : Planificateur de tÃ¢ches
- `airflow-worker` : ExÃ©cuteur de tÃ¢ches (Celery)
- `airflow-init` : Initialisation et crÃ©ation user admin

## ğŸ“Š ModÃ¨le de donnÃ©es

### PipelineExecution
Champs importants pour Airflow :
- `airflow_dag_run_id`: ID de l'exÃ©cution Airflow
- `status`: Ã‰tat mappÃ© depuis Airflow (pending, running, success, failed, cancelled)
- `started_at`: Timestamp de dÃ©but (ISO format)
- `completed_at`: Timestamp de fin (ISO format)
- `duration_seconds`: DurÃ©e calculÃ©e

### Mapping des Ã©tats Airflow

| Ã‰tat Airflow | Ã‰tat LogiData AI |
|--------------|------------------|
| `queued`     | `pending`        |
| `running`    | `running`        |
| `success`    | `success`        |
| `failed`     | `failed`         |
| Autre        | `unknown`        |

## ğŸ§ª Tests Ã  effectuer

### Test 1: ExÃ©cution simple
```bash
# 1. CrÃ©er un pipeline simple (CSV â†’ Filter â†’ PostgreSQL)
# 2. ExÃ©cuter via API
curl -X POST http://localhost:8000/api/v1/pipelines/{id}/execute \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"params": {}, "trigger_type": "manual"}'

# 3. VÃ©rifier le DAG gÃ©nÃ©rÃ©
ls backend/dags/pipeline_*.py

# 4. VÃ©rifier dans Airflow UI
open http://localhost:8080

# 5. Monitorer l'exÃ©cution
curl -X POST http://localhost:8000/api/v1/executions/{execution_id}/monitor \
  -H "Authorization: Bearer $TOKEN"
```

### Test 2: Annulation
```bash
# 1. Lancer une exÃ©cution longue
# 2. Annuler via API
curl -X POST http://localhost:8000/api/v1/executions/{execution_id}/cancel \
  -H "Authorization: Bearer $TOKEN"

# 3. VÃ©rifier que le statut passe Ã  "cancelled"
```

### Test 3: Pipeline complexe
```bash
# 1. CrÃ©er un pipeline avec plusieurs transformateurs
# 2. VÃ©rifier les dÃ©pendances dans le DAG gÃ©nÃ©rÃ©
# 3. VÃ©rifier le passage de donnÃ©es via XCom
```

## ğŸš€ Prochaines Ã©tapes

### Court terme
- [ ] Tester l'intÃ©gration complÃ¨te avec les services Docker
- [ ] Corriger les bugs Ã©ventuels
- [ ] Ajouter la gestion des logs depuis Airflow
- [ ] ImplÃ©menter le monitoring automatique pÃ©riodique

### Moyen terme
- [ ] WebSocket pour le monitoring en temps rÃ©el
- [ ] MÃ©triques dÃ©taillÃ©es par tÃ¢che
- [ ] Retry automatique des tÃ¢ches Ã©chouÃ©es
- [ ] Alertes en cas d'Ã©chec

### Long terme
- [ ] Support des DAGs conditionnels (branches)
- [ ] Support des DAGs avec boucles
- [ ] Optimisation des performances
- [ ] Gestion avancÃ©e des ressources

## âš ï¸ Points d'attention

### 1. SÃ©rialisation XCom
Les DataFrames Pandas doivent Ãªtre convertis en dict pour passer via XCom :
```python
result_dict = {
    "data": df.to_dict(orient="records"),
    "columns": df.columns.tolist(),
    "shape": df.shape,
}
```

### 2. Import des modules
Le DAG gÃ©nÃ©rÃ© doit pouvoir importer les modules depuis `/app` :
```python
sys.path.insert(0, '/app')
```

### 3. DÃ©lai de dÃ©tection des DAGs
AprÃ¨s gÃ©nÃ©ration, attendre 2 secondes pour qu'Airflow dÃ©tecte le nouveau DAG :
```python
time.sleep(2)
```

### 4. Gestion des erreurs
- Les erreurs dans les modules ETL doivent remonter Ã  Airflow
- Le statut doit Ãªtre mis Ã  jour en cas d'Ã©chec
- Les logs doivent Ãªtre conservÃ©s

## ğŸ“– Ressources

- [Documentation Airflow](https://airflow.apache.org/docs/)
- [Airflow REST API](https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html)
- [Custom Operators](https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.html)
- [XCom](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.html)

## ğŸ¤ Contribution

Pour toute modification de l'intÃ©gration Airflow :
1. Documenter les changements dans ce fichier
2. Tester avec plusieurs types de pipelines
3. VÃ©rifier la compatibilitÃ© avec les versions Airflow 2.8+

---

**Auteur:** LogiData AI Team
**Date:** 2024-11-19
**Version:** 1.0.0
